import os
import psycopg2
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# ØªØ­Ù…ÙŠÙ„ .env
load_dotenv()

print("=" * 60)
print("Starting Embeddings Builder")
print("=" * 60)

DB_HOST = os.getenv("NEON_DB_HOST")
DB_PORT = os.getenv("NEON_DB_PORT", "5432")
DB_NAME = os.getenv("NEON_DB_NAME")
DB_USER = os.getenv("NEON_DB_USER")
DB_PASSWORD = os.getenv("NEON_DB_PASSWORD")
DB_SSLMODE = os.getenv("NEON_DB_SSLMODE", "require")

EMBED_MODEL_NAME = os.getenv(
    "EMBED_MODEL", "Omartificial-Intelligence-Space/arabic-matryoshka-embed-base"
)

print(f"\nğŸ“¦ Loading embedding model: {EMBED_MODEL_NAME}")
try:
    embed_model = SentenceTransformer(EMBED_MODEL_NAME)
    print("âœ… Model loaded successfully")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    print("âš ï¸  Falling back to multilingual model...")
    embed_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

def get_connection():
    from app.database import get_connection as get_conn
    return get_conn()

def return_connection(conn):
    from app.database import return_connection as return_conn
    return_conn(conn)

def clear_embeddings():
    print("\nğŸ—‘ï¸  Clearing old embeddings...")
    try:
        conn = get_connection()
        cur = conn.cursor()
        cur.execute("DELETE FROM documents_embeddings;")
        deleted_count = cur.rowcount
        conn.commit()
        cur.close()
        return_connection(conn)
        print(f"âœ… Deleted {deleted_count} old embeddings")
    except Exception as e:
        print(f"âŒ Error clearing embeddings: {e}")

def fetch_trips_with_stops():
    """
    ÙŠØ¬Ù„Ø¨ ÙƒÙ„ Ø±Ø­Ù„Ø© Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ¹ÙˆØ¯ Ø§Ù„ØªØ§Ø¨Ø¹Ø© Ù„Ù‡Ø§
    """
    print("\nğŸ“Š Fetching trips data...")
    conn = get_connection()
    cur = conn.cursor()
    try:
        cur.execute(
            """
            SELECT
                t.trip_id,
                r.origin_city,
                r.destination_city,
                t.departure_time,
                t.arrival_time,
                t.base_price,
                t.status,
                COALESCE(
                    STRING_AGG(
                        CONCAT(
                            'Ù†Ù‚Ø·Ø© ØµØ¹ÙˆØ¯: ', rs.stop_name,
                            ' ÙÙŠ Ù…ÙˆÙ‚Ø¹ ', rs.stop_location,
                            ' - ØªØ±ØªÙŠØ¨: ', rs.stop_order
                        ),
                        ' | '
                        ORDER BY rs.stop_order
                    ),
                    ''
                ) AS boarding_points
            FROM trips t
            LEFT JOIN routes r ON r.route_id = t.route_id
            LEFT JOIN route_stops rs ON rs.route_id = t.route_id
            WHERE t.status = 'Scheduled'
            GROUP BY
                t.trip_id,
                r.origin_city,
                r.destination_city,
                t.departure_time,
                t.arrival_time,
                t.base_price,
                t.status;
            """
        )
        rows = cur.fetchall()
        print(f"âœ… Found {len(rows)} scheduled trips")
    except Exception as e:
        print(f"âŒ Error fetching trips: {e}")
        rows = []
    cur.close()
    return_connection(conn)
    return rows

def fetch_routes():
    """
    ÙŠØ¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
    """
    print("\nğŸ“Š Fetching routes data...")
    conn = get_connection()
    cur = conn.cursor()
    try:
        cur.execute(
            """
            SELECT
                r.route_id,
                r.origin_city,
                r.destination_city,
                r.estimated_duration_hours,
                r.distance_km,
                COALESCE(
                    STRING_AGG(
                        CONCAT(
                            rs.stop_name,
                            ' (', rs.stop_location, ')',
                            ' - ØªØ±ØªÙŠØ¨: ', rs.stop_order
                        ),
                        ' | '
                        ORDER BY rs.stop_order
                    ),
                    'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù‚Ø§Ø· ØªÙˆÙ‚Ù'
                ) AS route_stops
            FROM routes r
            LEFT JOIN route_stops rs ON rs.route_id = r.route_id
            GROUP BY
                r.route_id,
                r.origin_city,
                r.destination_city,
                r.estimated_duration_hours,
                r.distance_km;
            """
        )
        rows = cur.fetchall()
        print(f"âœ… Found {len(rows)} routes")
    except Exception as e:
        print(f"âŒ Error fetching routes: {e}")
        rows = []
    cur.close()
    return_connection(conn)
    return rows

def fetch_cancel_policies():
    """
    ÙŠØ¬Ù„Ø¨ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø¥Ù„ØºØ§Ø¡ Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ©
    """
    print("\nğŸ“Š Fetching cancellation policies...")
    conn = get_connection()
    cur = conn.cursor()
    try:
        cur.execute("""
            SELECT 
                cp.cancel_policy_id, 
                cp.policy_name, 
                cp.description, 
                cp.refund_percentage, 
                cp.days_before_trip,
                p.company_name
            FROM cancel_policies cp
            LEFT JOIN partners p ON p.partner_id = cp.partner_id;
        """)
        rows = cur.fetchall()
        print(f"âœ… Found {len(rows)} cancellation policies")
    except Exception as e:
        print(f"âŒ Error fetching policies: {e}")
        rows = []
    cur.close()
    return_connection(conn)
    return rows

def insert_embedding(source_table, source_id, text_chunk, embedding):
    try:
        conn = get_connection()
        cur = conn.cursor()
        embedding_str = "[" + ",".join(str(float(x)) for x in embedding) + "]"
        cur.execute(
            """
            INSERT INTO documents_embeddings (source_table, source_id, text_chunk, embedding)
            VALUES (%s, %s, %s, %s::vector)
            ON CONFLICT DO NOTHING;
            """,
            (source_table, source_id, text_chunk, embedding_str),
        )
        conn.commit()
        cur.close()
        return_connection(conn)
        return True
    except Exception as e:
        print(f"  âŒ Error inserting embedding for {source_table}#{source_id}: {e}")
        return False

def index_trips():
    rows = fetch_trips_with_stops()
    print(f"\nğŸ”„ Indexing {len(rows)} trips...")
    success_count = 0
    
    for idx, row in enumerate(rows, 1):
        (
            trip_id,
            origin_city,
            destination_city,
            departure_time,
            arrival_time,
            base_price,
            status,
            boarding_points,
        ) = row

        text_chunk = (
            f"Ø±Ø­Ù„Ø© Ø±Ù‚Ù… {trip_id}.\n"
            f"ØªÙ†Ø·Ù„Ù‚ Ù…Ù† Ù…Ø¯ÙŠÙ†Ø© {origin_city or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'} Ø¥Ù„Ù‰ Ù…Ø¯ÙŠÙ†Ø© {destination_city or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}.\n"
            f"ÙˆÙ‚Øª Ø§Ù„Ù…ØºØ§Ø¯Ø±Ø©: {departure_time}ØŒ ÙˆÙˆÙ‚Øª Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {arrival_time}.\n"
            f"Ø³Ø¹Ø± Ø§Ù„ØªØ°ÙƒØ±Ø©: {base_price} Ø±ÙŠØ§Ù„.\n"
            f"Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ø­Ù„Ø©: {status}.\n"
            f"Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ¹ÙˆØ¯ Ø§Ù„Ù…ØªØ§Ø­Ø©: {boarding_points or 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù‚Ø§Ø· ØµØ¹ÙˆØ¯ Ø¥Ø¶Ø§ÙÙŠØ©'}."
        )

        emb = embed_model.encode(text_chunk)
        if insert_embedding("trips", trip_id, text_chunk, emb):
            success_count += 1
        
        if idx % 10 == 0:
            print(f"  Progress: {idx}/{len(rows)} trips indexed")
    
    print(f"âœ… Successfully indexed {success_count}/{len(rows)} trips")

def index_routes():
    rows = fetch_routes()
    print(f"\nğŸ”„ Indexing {len(rows)} routes...")
    success_count = 0
    
    for idx, row in enumerate(rows, 1):
        (
            route_id,
            origin_city,
            destination_city,
            estimated_duration_hours,
            distance_km,
            route_stops,
        ) = row

        text_chunk = (
            f"Ù…Ø³Ø§Ø± Ø±Ù‚Ù… {route_id}.\n"
            f"Ù…Ù† {origin_city or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'} Ø¥Ù„Ù‰ {destination_city or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}.\n"
            f"Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {estimated_duration_hours or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'} Ø³Ø§Ø¹Ø©.\n"
            f"Ø§Ù„Ù…Ø³Ø§ÙØ©: {distance_km or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'} ÙƒÙ….\n"
            f"Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø±: {route_stops}"
        )

        emb = embed_model.encode(text_chunk)
        if insert_embedding("routes", route_id, text_chunk, emb):
            success_count += 1
    
    print(f"âœ… Successfully indexed {success_count}/{len(rows)} routes")

def index_policies():
    rows = fetch_cancel_policies()
    print(f"\nğŸ”„ Indexing {len(rows)} cancellation policies...")
    success_count = 0
    
    for row in rows:
        policy_id, policy_name, description, refund_percentage, days_before, company_name = row
        text_chunk = (
            f"Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ù„ØºØ§Ø¡: {policy_name}.\\n"
            f"Ø§Ù„Ø´Ø±ÙƒØ©: {company_name or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}.\\n"
            f"{description or 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ'}.\\n"
            f"Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {refund_percentage}%.\\n"
            f"ÙŠØ¬Ø¨ Ø§Ù„Ø¥Ù„ØºØ§Ø¡ Ù‚Ø¨Ù„ {days_before} ÙŠÙˆÙ… Ù…Ù† Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø±Ø­Ù„Ø©."
        )
        emb = embed_model.encode(text_chunk)
        if insert_embedding("cancel_policies", policy_id, text_chunk, emb):
            success_count += 1
    
    print(f"âœ… Successfully indexed {success_count}/{len(rows)} policies")

def fetch_active_faqs():
    """
    ÙŠØ¬Ù„Ø¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„Ù†Ø´Ø·Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    print("\nğŸ“Š Fetching active FAQs...")
    conn = get_connection()
    cur = conn.cursor()
    try:
        cur.execute("""
            SELECT 
                faq_id, 
                category, 
                question, 
                answer 
            FROM faqs 
            WHERE is_active = true
            ORDER BY display_order;
        """)
        rows = cur.fetchall()
        print(f"âœ… Found {len(rows)} active FAQs")
    except Exception as e:
        print(f"âŒ Error fetching FAQs: {e}")
        rows = []
    cur.close()
    return_connection(conn)
    return rows

def index_faqs():
    rows = fetch_active_faqs()
    print(f"\nğŸ”„ Indexing {len(rows)} FAQs...")
    success_count = 0
    
    for row in rows:
        faq_id, category, question, answer = row
        text_chunk = (
            f"Ø³Ø¤Ø§Ù„ Ø´Ø§Ø¦Ø¹: {question}\n"
            f"Ø§Ù„ØªØµÙ†ÙŠÙ: {category or 'Ø¹Ø§Ù…'}\n"
            f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {answer}"
        )
        
        # Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ + Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù„Ø¶Ù…Ø§Ù† Ø¯Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«
        emb = embed_model.encode(text_chunk)
        if insert_embedding("faqs", faq_id, text_chunk, emb):
            success_count += 1
            
    print(f"âœ… Successfully indexed {success_count}/{len(rows)} FAQs")

def main():
    try:
        clear_embeddings()
        index_trips()
        index_routes()
        index_policies()
        index_faqs()
        print("\n" + "=" * 60)
        print("âœ… Indexing completed successfully!")
        print("=" * 60)
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
